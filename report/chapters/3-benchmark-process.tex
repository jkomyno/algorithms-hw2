\section{Benchmark}
\label{cap:benchmark-process}

Abbiamo deciso di rendere il processo di misurazione del tempo di
esecuzione dei nostri algoritmi esterno al codice dei programmi
sviluppati, realizzando un script in grado di misurare i tempi di
esecuzione. Per l'analisi dei dati invece abbiamo scritto uno script
in Python, in grado di elaborare i risultati e produrre tabelle e
grafici.

\subsection{Misurazione}

Il tempo di esecuzione dell'algoritmo, essendo esterno al codice,
tiene conto di diversi fattori:

\begin{itemize}
    \item Tempo necessario a leggere il file di input in un container
      \textit{std::vector} temporaneo;
    \item Tempo per trasformare il container temporaneo in una Matrice
      delle Distanze;
    \item Tempo per eseguire l'algoritmo vero e proprio per la
      risoluzione di TSP e restituire il risultato;
\end{itemize}

\noindent Il processo di misurazione restituisce un file in formato
CSV che riassume il risultato del benchmark. In particolare, il file
contiene le seguenti colonne:

\begin{itemize}
    \item \textbf{ms}: tempo in millisecondi per eseguire il programma
      su un singolo file di input;
    \item \textbf{output}: risultato del programma, ovvero il peso
      del circuito Hamiltoniano (ottimo o approssimato) individuato sul grafo dato in input;
    \item \textbf{d}: numero di nodi del grafo di input;
    \item \textbf{weight\textunderscore type}: tipo delle distanze del
      grafo (\textit{EUC\_2D} o \textit{GEO}), i cui valori ammissibili sono descritti in \hyperref[sub:graph-representation]{Rappresentazione del Grafo};
    \item \textbf{filename}: nome del file di input letto.
\end{itemize}

\noindent Per rendere i risultati del benchmark quanto più stabili e
affidabili possibile, abbiamo preso le seguenti precauzioni:

\begin{itemize}
    \item Abbiamo usato sempre lo stesso computer per misurare il
      tempo di esecuzione dei programmi implementati;
    \item Abbiamo chiuso tutti i programmi in foreground e
      disabilitato quanti più servizi possibile in background;
    \item Abbiamo disabilitato la connessione Internet del computer
      scelto;
    \item Abbiamo fatto più misurazioni in tempi differenti. Di tutte
      le misurazioni effettuate è poi stata scelta la minima per
      elaborazioni e grafici.
\end{itemize}

\noindent Il computer usato per effettuare i benchmark degli algoritmi
ha le seguenti caratteristiche:

\begin{itemize}
    \item \textbf{Sistema Operativo}: Windows 10 Education 64 bit;
    \item \textbf{CPU}: Intel Core i5 6600K 3.50 GHz
    \item \textbf{RAM}: 16 GB;
\end{itemize}

\subsection{Analisi}

\noindent Abbiamo definito lo script Python
\textit{benchmark/analysis.py} per automatizzare il processo di
confronto degli algoritmi e la creazione di tabelle e grafici inseriti
in questa relazione. Esso legge i file CSV generati dallo script di
benchmark \textit{benchmark.sh}. \\

\noindent Lo script trasforma i dati grezzi in dati manipolabili e li
elabora estraendone le informazioni principali e mostrandole sotto
forma di grafici e tabelle. Di seguito sono riportate ad alto
livello le fasi eseguite dallo script:

\begin{enumerate}
    \item Lettura di tutti i file CSV e trasformazione in DataFrames \codeinline{Pandas};
    \item Esecuzione di controlli (asserzioni) sulla struttura dei dati
      letti e sul loro significato, per assicurare che CSV siano esenti da errori;
    \item Elaborazione dei dati. In particolare i benchmark vengono raggruppati
      per algoritmo in una singola tabella, e per ogni riga
      viene mantenuto il dato con il tempo di esecuzione minore. La
      colonna degli output invece mantiene il valore mediano tra tutti
      i benchmark per ogni algoritmo.
    \item Estrazione della conoscenza tramite la creazione di tabelle
      e grafici con semplici primitive integrate nello script.
    \label{script-phase-analysis}
\end{enumerate}

\noindent Le primitive utilizzate nella fase
\ref{script-phase-analysis} per la creazione di tabelle e grafici
sono:

\begin{itemize}
    \item \mintinline{python}{print_comparison}: Crea una tabella di
      comparativa che mostra l'errore introdotto dagli algoritmi rispetto alle soluzioni ottime.

    \item \mintinline{python}{plot_precision_comparison}: Crea un
      grafico che mostra l'errore introdotto dagli algoritmi rispetto alle soluzioni ottime.

    \item \mintinline{python}{plot_runtime_comparison}: Crea un
      grafico di che mostra il tempo di esecuzione degli algoritmi.

\end{itemize}

\noindent Le funzioni sono ampiamente documentate nello script, al
quale si rimanda per ulteriori dettagli.
